## Updates

### [January 2024] New Model Release: Arithmo2-Mistral-7B

**Arithmo2-Mistral-7B** model improves initially released Arithmo-Mistral-7B model on both GSM8K and MATH benchmarks. Specifically, there is absolute improvement of **+1.7% on GSM8K, +3.0% on GSM8K PoT, and +1.9% on MATH benchmarks**. We release both [merged model](https://huggingface.co/upaya07/Arithmo2-Mistral-7B) and [LoRA Adapter](https://huggingface.co/upaya07/Arithmo2-Mistral-7B-adapter).
- Arithmo2-Mistral-7B is trained on same data as Arithmo-Mistral-7B except that we removed both validation and test set of [lila ood subset](https://huggingface.co/datasets/allenai/lila/viewer/ood) to avoid possibility of data leakage.
- Added [NEFTune](https://arxiv.org/pdf/2310.05914.pdf)
- Enabled sample packing = true for faster training.


# Arithmo Models
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](LICENSE)
[![Model Weight License](https://img.shields.io/badge/Model%20Weights%20License-Apache_2.0-green.svg)](LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)


Both [Arithmo2-Mistral-7B](https://huggingface.co/upaya07/Arithmo2-Mistral-7B) and [Arithmo-Mistral-7B](https://huggingface.co/akjindal53244/Arithmo-Mistral-7B) models are trained to reason and answer mathematical problems and is also capable of writing a Python program that upon execution prints answer to the question. We used [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) as a base model and used **QLoRA to fine-tune it on a single RTX 4090 GPU**.


## Benchmark Results

Arithmo2-Mistral-7B model is fine-tuned with 4-bit QLoRA on single GPU and is competitive with supervised full-finetuned state-of-the-art Mathematical Reasoning models. Refer to [Comparing Arithmo models with other SFT LLM models](https://github.com/akjindal53244/Arithmo/tree/master?tab=readme-ov-file#comparing-arithmo-models-with-other-sft-llm-models) section for more details.

<table>
    <thead>
        <tr>
            <th>Model Name</th>
            <th>Checkpoint</th>
            <th>Training Approach</th>
            <th>Prompt Approach</th>
            <th>GSM8k</th>
            <th>MATH</th>
            <th>License</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=2>Arithmo-Mistral-7B</td>
            <td rowspan=2>ðŸ¤— <a href="https://huggingface.co/akjindal53244/Arithmo-Mistral-7B" target="_blank">Model</a></td>
            <td rowspan=2>4-bit QLoRA Fine-tuning on 1x4090</td>
            <td>Zero-Shot CoT</td>
            <td>74.7</td>
            <td>25.3</td>
            <td rowspan=2>Apache-2.0</td>
        </tr>
        <tr>
            <td>Zero-Shot PoT</td>
            <td>71.2</td>
            <td>-</td>
        </tr>
        <tr>
            <td rowspan=2>ðŸ”¥ <b>Arithmo2-Mistral-7B</b> </td>
            <td rowspan=2>ðŸ¤— <a href="https://huggingface.co/upaya07/Arithmo2-Mistral-7B" target="_blank">Model</a> <br> ðŸ¤— <a href="https://huggingface.co/upaya07/Arithmo2-Mistral-7B-adapter" target="_blank">LoRA Adapter</a> </td>
            <td rowspan=2>4-bit QLoRA Fine-tuning on 1x4090</td>
            <td>Zero-Shot CoT</td>
            <td><b>76.4</b></td>
            <td><b>27.2</b></td>
            <td rowspan=2>Apache-2.0</td>
        </tr>
        <tr>
            <td>Zero-Shot PoT</td>
            <td><b>74.2</b></td>
            <td>-</td>
        </tr>
    </tbody>
</table>

- **Zero-Shot CoT**: On providing a question as prompt, model generates reasoning steps to solve the question along with answer. We check if answer matches with ground-truth.
- **Zero-Shot PoT**: We prompt the model to generate a Python program for the given question. During inference, we execute the Python program generated by the model and check if the program output matches with ground-truth answer. Visit [Model Card](https://huggingface.co/akjindal53244/Arithmo-Mistral-7B) to see few PoT examples.


## Installation

```
pip install transformers >=4.34.0
pip install accelerate
pip install sentencepiece
pip install protobuf

# If you are GPU poor like me
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# If you have a GPU.
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118
pip install scipy
pip install bitsandbytes
```


## How to query the model

```
# Set `run_model_on_gpu` to `False` if you are running on CPU. Model will generate reasoning steps with answer for your question. If you want to generate Python program, uncomment line-69 that adds a Python prompt.
# This script automatically does formatting for you, so you just need to type question (eg: `What is 2+2?`) without any prefix like `Question:`, etc.**

$ python query_model.py
```
**Note:** Above script automatically does formatting for you, so you just need to type question (eg: `What is 2+2?`) without any prefix like `Question:`, etc. Checkout `query_model.py` for more details. <br><br>

##### Sample Input:
```
Question: There are total 10 children. I have to give 1 apple to first child, 2 apples to second child, 3 apples to third child, and so on. How many apples do I need?
```
##### Model Output:
```
Answer: The total number of apples needed is the sum of the first 10 positive integers.
This can be calculated using the formula for the sum of an arithmetic series:
\[S = \frac{n}{2}(a_1 + a_n),\]
where $S$ is the sum, $n$ is the number of terms, $a_1$ is the first term, and $a_n$ is the last term.
In this case, $n = 10$, $a_1 = 1$, and $a_n = 10$.
Plugging these values into the formula, we get:
\[S = \frac{10}{2}(1 + 10) = 5(11) = \boxed{55}.\]
The answer is: 55
```

Arithmo-Mistral-7B is trained with the following format:
#### CoT Format (generate reasoning steps with answer):
```
Question: <question>

Answer:
```

#### PoT Format (generate a python program):
```
Question: <question> <python_prompt>

Answer:
```
It will perform best if queried in this way with your own script.


## Model Finetuning Details
Due to limited compute budget, Mistral-7B model is fine-tuned with QLoRA using Single RTX 4090 GPU. We plan to do a full finetuning of Mistral-7B model on this dataset to further improve performance. <br>
<br>

## Reproducing Results

### Model Training Data
Model training data is prepared by combining [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA) (train split), [lila OOD](https://huggingface.co/datasets/allenai/lila/viewer/ood) (train, validation, and test splits), and [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct) (train split) datasets. We have verified that our training data has no overlap with GSM8K and MATH test set. Further post-processing steps are applied such as 1) deduplication, 2) randomly lower-casing x% inputs, 3) adding diverse set of Python prompts for PoT, and 4) standardizing answer format. Final dataset is of size ~540,000. Also, to train Arithmo2-Mistral-7B model, we removed both validation and test set of [lila ood subset](https://huggingface.co/datasets/allenai/lila/viewer/ood) to avoid possibility of data leakage.

```
# This script generates train and eval sets.
$ python data_prep/prepare_model_traininig_data.py
```

Here is [Huggingface link](https://huggingface.co/datasets/akjindal53244/Arithmo-Data) for our dataset.

### Answer/Response Generation

#### Prediction on [GSM8K Test set](https://huggingface.co/datasets/gsm8k/viewer/main/test)
##### Zero-Shot with CoT:
```
# This script saves output to `data/predictions/gsm8k/Arithmo-Mistral-7B/predictions_Arithmo_gsm8k_zero_shot_CoT.json` path.
$ python eval/gsm8k/gsm8k_generate_response_zero_shot_CoT.py
```

##### Zero-Shot with PoT:
```
# This script saves output to `data/predictions/gsm8k/Arithmo-Mistral-7B/predictions_Arithmo_gsm8k_zero_shot_PoT.json` path.
$ python eval/gsm8k/gsm8k_generate_response_zero_shot_PoT.py
```

#### Prediction on [MATH Test set](https://huggingface.co/datasets/competition_math/viewer/default/test)
##### Zero-Shot with CoT:
```
# This script saves output to `data/predictions/gsm8k/Arithmo-Mistral-7B/predictions_Arithmo_MATH_zero_shot_CoT.json` path.
$ python eval/MATH/MATH_generate_response_zero_shot_CoT.py
```

**Zero-Shot with PoT**: Answers in MATH test set consist of expressions like `(x+2)/5` instead of a numeric value. Currently, Arithmo-Mistral-7B's PoT training data doesn't contain expressions as answers. Hence, we don't run PoT based inference on MATH dataset.


### Metrics Computation

#### [GSM8K Test set](https://huggingface.co/datasets/gsm8k/viewer/main/test)
##### Zero-Shot with CoT:
```
$ python eval/gsm8k/gsm8k_compute_metric_zero_shot_CoT.py
```
Expected output: `Total Instances: 1319, Correct Count: 985, Accuracy (Correct Count/Total Instances): 0.7467` <br> <br>
##### Zero-Shot with PoT:
```
# Step-1: This script executes generated python programs and saves results into a file.
$ python eval/gsm8k/gsm8k_write_zero_shot_PoT_outputs.py > data/predictions/gsm8k/Arithmo-Mistral-7B/gsm8k_zero_shot_PoT_results.txt

# Step-2: This script computes accuracy by taking above file as input.
$ python eval/gsm8k/gsm8k_compute_metric_zero_shot_PoT.py
```
Expected output: `Total Instances: 1309, Correct Count: 932, Accuracy: 0.7119`

#### [MATH Test set](https://huggingface.co/datasets/competition_math/viewer/default/test)
##### Zero-Shot with CoT:
```
$ python eval/MATH/MATH_compute_metric_zero_shot_CoT.py
```
Script is borrowed from official [math repository](https://github.com/hendrycks/math/blob/main/modeling/math_equivalence.py) <br>
Expected output: `Total Instances: 5000, Correct Count: 1266, Accuracy (Correct Count/Total Instances): 0.2532`


## Comparing Arithmo models with other SFT LLM models
Results for all models except `Arithmo2-Mistral-7B` and `Arithmo-Mistral-7B` are taken from [MetaMath](https://github.com/meta-math/MetaMath/blob/main/README.MD) repository.

| Model               | GSM8k Pass@1 | MATH Pass@1 | Model Training details |
|---------------------|--------------|-------------|------------------------|
| MPT-7B              | 6.8          | 3.0         |
| Falcon-7B           | 6.8          | 2.3         |
| LLaMA-1-7B          | 11.0         | 2.9         |
| LLaMA-2-7B          | 14.6         | 2.5         |
| MPT-30B             | 15.2         | 3.1         |
| LLaMA-1-13B         | 17.8         | 3.9         |
| GPT-Neo-2.7B        | 19.5         | --          |
| Falcon-40B          | 19.6         | 2.5         |
| Baichuan-chat-13B   | 23.9         | --          |
| Vicuna-v1.3-13B     | 27.6         | --          |
| LLaMA-2-13B         | 28.7         | 3.9         |
| InternLM-7B         | 31.2         | --          |
| ChatGLM-2-6B        | 32.4         | --          |
| GPT-J-6B            | 34.9         | --          |
| LLaMA-1-33B         | 35.6         | 3.9         |
| LLaMA-2-34B         | 42.2         | 6.24        |
| RFT-7B              | 50.3         | --          |
| LLaMA-1-65B         | 50.9         | 10.6        |
| Qwen-7B             | 51.6         | --          |
| WizardMath-7B       | 54.9         | 10.7        |
| LLaMA-2-70B         | 56.8         | 13.5        |
| WizardMath-13B      | 63.9         | 14.0        |
| MetaMath-7B         | 66.5         | 19.8        |
| MetaMath-13B        | 72.3         | 22.4        |
| Arithmo-Mistral-7B (PoT)  | 71.2 | --       | SFT: 4-bit QLoRA |
| Arithmo2-Mistral-7B (PoT)  | 74.2 | --       | SFT: 4-bit QLoRA |
| MetaMath-Mistral-7B  | 77.7 | 28.2       | SFT: Full fine-tuned |
| Arithmo-Mistral-7B| 74.7 | 25.3       | SFT: 4-bit QLoRA |
| ðŸ”¥ **Arithmo2-Mistral-7B**  | **76.4** | **27.2**       | **SFT: 4-bit QLoRA** |


### Citation
To cite Arithmo models:
```
@misc{jindal_2023_arithmo,
  author = {Jindal, Ashvini},
  title = {Arithmo-Mistral-7B: Mathematical Reasoning Model},
  howpublished = {Hugging Face},
  month = {October},
  year = {2023},
  url = {https://huggingface.co/akjindal53244/Arithmo-Mistral-7B}
}
```

### Support My Work
Building LLMs takes time and resources; if you find my work interesting, your support would be epic! <br>

<a href="https://www.buymeacoffee.com/a_little_learner" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>

P.S.: If you are interested in providing compute support, please reach out to [Ashvini Jindal](https://www.linkedin.com/in/ashvini-jindal-26653262/)


<h2 id="References">References</h2>

```
@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{Yue2023mammoth,
  title={MAmmoTH: Building math generalist models through hybrid instruction tuning},
  author={Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}

@article{mishra2022lila,
  title={Lila: A unified benchmark for mathematical reasoning},
  author={Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan},
  journal={arXiv preprint arXiv:2210.17517},
  year={2022}
}
```

## Todos
- 
