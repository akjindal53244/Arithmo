# Arithmo-Mistral-7B
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](LICENSE)
[![Model Weight License](https://img.shields.io/badge/Model%20Weights%20License-Apache_2.0-green.svg)](LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

[Arithmo-Mistral-7B](https://huggingface.co/akjindal53244/Arithmo-Mistral-7B) is an instruction-tuned [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) model using QLoRA on single RTX 4090 GPU. Model is tuned to reason and answer mathematical problems and is also capable of writing a Python program that upon execution prints answer to the question.


## Results

Arithmo-Mistral-7B outperforms existing 7B and 13B state-of-the-art Mathematical Reasoning models. Refer to [Comparing Arithmo-Mistral-7B with other LLM models](https://github.com/akjindal53244/Arithmo-Mistral-7B/tree/master#comparing-arithmo-mistral-7b-with-other-llm-models) section for more details.

<table>
    <thead>
        <tr>
            <th>Model Name</th>
            <th>Checkpoint</th>
            <th>Prompt Approach</th>
            <th>GSM8k</th>
            <th>MATH</th>
            <th>License</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=2>Arithmo-Mistral-7B</td>
            <td rowspan=2>ðŸ¤— <a href="https://huggingface.co/akjindal53244/Arithmo-Mistral-7B" target="_blank">Huggingface Link</a></td>
            <td>Zero-Shot CoT</td>
            <td><b>74.7</b></td>
            <td><b>25.3</b></td>
            <td rowspan=2>Apache-2.0</td>
        </tr>
        <tr>
            <td>Zero-Shot PoT</td>
            <td><b>71.2</b></td>
            <td>-</td>
        </tr>
    </tbody>
</table>

- **Zero-Shot CoT**: On providing a question as prompt, model generates reasoning steps to solve the question along with answer. We check if answer matches with ground-truth.
- **Zero-Shot PoT**: We prompt the model to generate a Python program for the given question. During inference, we execute the Python program generated by the model and check if the program output matches with ground-truth answer. Visit [Model Card](https://huggingface.co/akjindal53244/Arithmo-Mistral-7B) to see few PoT examples.


## Installation

```
# If you are GPU poor like me
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# If you have a GPU
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118

pip install transformers
pip install accelerate
pip install sentencepiece
pip install protobuf
```


## How to query the model

```
# Set `run_model_on_gpu` to `False` if you are running on CPU. Model will generate reasoning steps with answer for your question. If you want to generate Python program, uncomment line-69 that adds a Python prompt.
# This script automatically does formatting for you, so you just need to type question (eg: `What is 2+2?`) without any prefix like `Question:`, etc.**

$ python query_model.py
```
**Note:** Above script automatically does formatting for you, so you just need to type question (eg: `What is 2+2?`) without any prefix like `Question:`, etc. Checkout `query_model.py` for more details. <br><br>


Arithmo-Mistral-7B is trained with the following format:
#### CoT Format (generate reasoning steps with answer):
```
Question: <question>

Answer:
```

#### PoT Format (generate a python program):
```
Question: <question> <python_prompt>

Answer:
```
It will perform best if queried in this way with your own script.


## Model Finetuning Details
Due to limited compute budget, Mistral-7B model is instruction-tuned with QLoRA using Single RTX 4090 GPU. We plan to do a full finetuning of Mistral-7B model on this dataset to further improve performance. <br>
<br>
**P.S.:** Please reach out to [Ashvini Jindal](https://www.linkedin.com/in/ashvini-jindal-26653262/) if you would be interested in supporting compute need. We are looking for small-scale support so we'd appreciate any kind of help! :)

## Reproducing Results

### Model Training Data
Model training data is prepared by combining [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA), [lila OOD](https://huggingface.co/datasets/allenai/lila/viewer/ood) and [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct) datasets. Further post-processing steps are applied such as 1) deduplication, 2) randomly lower-casing x% inputs, 3) adding diverse set of Python prompts for PoT, and 4) standardizing answer format. Final dataset is of size ~540,000.
```
# This script generates train and eval sets.
$ python data_prep/prepare_model_traininig_data.py
```

### Answer/Response Generation

#### Prediction on [GSM8K Test set](https://huggingface.co/datasets/gsm8k/viewer/main/test)
##### Zero-Shot with CoT:
```
# This script saves output to `data/predictions/gsm8k/Arithmo-Mistral-7B/predictions_Arithmo_gsm8k_zero_shot_CoT.json` path.
$ python eval/gsm8k/gsm8k_generate_response_zero_shot_CoT.py
```

##### Zero-Shot with PoT:
```
# This script saves output to `data/predictions/gsm8k/Arithmo-Mistral-7B/predictions_Arithmo_gsm8k_zero_shot_PoT.json` path.
$ python eval/gsm8k/gsm8k_generate_response_zero_shot_PoT.py
```

#### Prediction on [MATH Test set](https://huggingface.co/datasets/competition_math/viewer/default/test)
##### Zero-Shot with CoT:
```
# This script saves output to `data/predictions/gsm8k/Arithmo-Mistral-7B/predictions_Arithmo_MATH_zero_shot_CoT.json` path.
$ python eval/MATH/MATH_generate_response_zero_shot_CoT.py
```

**Zero-Shot with PoT**: Answers in MATH test set consist of expressions like `(x+2)/5` instead of a numeric value. Currently, Arithmo-Mistral-7B's PoT training data doesn't contain expressions as answers. Hence, we don't run PoT based inference on MATH dataset.


### Metrics Computation

#### [GSM8K Test set](https://huggingface.co/datasets/gsm8k/viewer/main/test)
##### Zero-Shot with CoT:
```
$ python eval/gsm8k/gsm8k_compute_metric_zero_shot_CoT.py
```
Expected output: `Total Instances: 1319, Correct Count: 985, Accuracy (Correct Count/Total Instances): 0.7467` <br> <br>
##### Zero-Shot with PoT:
```
# Step-1: This script executes generated python programs and saves results into a file.
$ python eval/gsm8k/gsm8k_write_zero_shot_PoT_outputs.py > data/predictions/gsm8k/Arithmo-Mistral-7B/gsm8k_zero_shot_PoT_results.txt

# Step-2: This script computes accuracy by taking above file as input.
$ python eval/gsm8k/gsm8k_compute_metric_zero_shot_PoT.py
```
Expected output: `Total Instances: 1309, Correct Count: 932, Accuracy: 0.7119`

#### [MATH Test set](https://huggingface.co/datasets/competition_math/viewer/default/test)
##### Zero-Shot with CoT:
```
$ python eval/MATH/MATH_compute_metric_zero_shot_CoT.py
```
Script is borrowed from official [math repository](https://github.com/hendrycks/math/blob/main/modeling/math_equivalence.py) <br>
Expected output: `Total Instances: 5000, Correct Count: 1266, Accuracy (Correct Count/Total Instances): 0.2532`


## Comparing Arithmo-Mistral-7B with other LLM models.
Results for all models except `Arithmo-Mistral-7B` are taken from [MetaMath](https://github.com/meta-math/MetaMath/blob/main/README.MD) repository.

| Model               | GSM8k Pass@1 | MATH Pass@1 |
|---------------------|--------------|-------------|
| MPT-7B              | 6.8          | 3.0         |
| Falcon-7B           | 6.8          | 2.3         |
| LLaMA-1-7B          | 11.0         | 2.9         |
| LLaMA-2-7B          | 14.6         | 2.5         |
| MPT-30B             | 15.2         | 3.1         |
| LLaMA-1-13B         | 17.8         | 3.9         |
| GPT-Neo-2.7B        | 19.5         | --          |
| Falcon-40B          | 19.6         | 2.5         |
| Baichuan-chat-13B   | 23.9         | --          |
| Vicuna-v1.3-13B     | 27.6         | --          |
| LLaMA-2-13B         | 28.7         | 3.9         |
| InternLM-7B         | 31.2         | --          |
| ChatGLM-2-6B        | 32.4         | --          |
| GPT-J-6B            | 34.9         | --          |
| LLaMA-1-33B         | 35.6         | 3.9         |
| LLaMA-2-34B         | 42.2         | 6.24        |
| RFT-7B              | 50.3         | --          |
| LLaMA-1-65B         | 50.9         | 10.6        |
| Qwen-7B             | 51.6         | --          |
| WizardMath-7B       | 54.9         | 10.7        |
| LLaMA-2-70B         | 56.8         | 13.5        |
| WizardMath-13B      | 63.9         | 14.0        |
| MetaMath-7B         | 66.5         | 19.8        |
| MetaMath-13B        | 72.3         | 22.4        |
| ðŸ”¥ **Arithmo-Mistral-7B Zero-Shot PoT**  | **71.2** | --       |
| ðŸ”¥ **Arithmo-Mistral-7B Zero-Shot CoT**  | **74.7** | **25.3**       |
| WizardMath-70B      | **81.6**     | 22.7        |
| MetaMath-70B        | **82.3**     | **26.6**        |


## Todos

- 

## Sample Generation from Arithmo-Mistral-7B model
![image](https://github.com/akjindal53244/Arithmo-Mistral-7B/assets/5215386/e544bf81-0f1a-4f7b-aa35-9b1d90b31c6e)
